{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvmA/nYxnwWXAIaxZ+1oPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/teachingMLDL/blob/main/06.Probability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> *The laws of probability, so true in general, so fallacious in particular.\"*  \n",
        "> —Edward Gibbon\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zkskpNaHy9qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is hard to do data science without some sort of understanding of probability and its mathematics. As with our treatment of statistics in Chapter 5, we’ll wave our hands a lot and elide many of the technicalities.\n",
        "\n",
        "For our purposes you should think of probability as a way of quantifying the uncertainty associated with events chosen from some universe of events. Rather than get ting technical about what these terms mean, think of rolling a die. The universe consists of all possible outcomes. And any subset of these outcomes is an event; for example, “the die rolls a 1” or “the die rolls an even number.”\n",
        "\n",
        "Notationally, we write P(E) to mean “the probability of the event E.”\n",
        "We’ll use probability theory to build models. We’ll use probability theory to evaluate models. We’ll use probability theory all over the place.\n",
        "One could, were one so inclined, get really deep into the philosophy of what probability theory means. (This is best done over beers.) We won’t be doing that."
      ],
      "metadata": {
        "id": "bFQP8xeizFOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependence and Independence (Probability Events)"
      ],
      "metadata": {
        "id": "UKiDNs3yzVw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Summary: Dependence and Independence (Probability Events)\n",
        "\n",
        "**Why Dependence/Independence?**\n",
        "In probability, we often want to know whether two events are connected. If learning about one event changes what we believe about the other, they are **dependent**. If not, they are **independent**.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Definition: Dependent vs Independent Events**\n",
        "\n",
        "* **Dependent:** knowing whether event **E** happened gives information about whether **F** happened (and vice versa)\n",
        "* **Independent:** knowing **E** happened gives **no information** about **F**\n",
        "\n",
        "**2. Example: Two Coin Flips**\n",
        "\n",
        "* Event: “first flip is heads”\n",
        "\n",
        "  * gives no information about “second flip is heads”\n",
        "    → these are **independent**\n",
        "\n",
        "* Event: “first flip is heads”\n",
        "\n",
        "  * gives information about “both flips are tails”\n",
        "    → these are **dependent**\n",
        "    (because if the first flip is heads, then “both tails” is impossible)\n",
        "\n",
        "**3. Mathematical Condition for Independence**\n",
        "Two events **E** and **F** are independent if:\n",
        "$$\n",
        "P(E \\cap F) = P(E)\\,P(F)\n",
        "$$\n",
        "Meaning:\n",
        "\n",
        "\n",
        "* probability both happen = probability of (E) × probability of (F)\n",
        "\n",
        "**4. Coin Flip Probability Check**\n",
        "\n",
        "- $P(\\text{first flip heads}) = \\frac{1}{2}$  \n",
        "- $P(\\text{both flips tails}) = \\frac{1}{4}$  \n",
        "- $P(\\text{first flip heads AND both tails}) = 0$  \n",
        "\n",
        "Since:\n",
        "\n",
        "- $0 \\neq \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{4}\\right)$  \n",
        "  these events are **not independent** (they are dependent).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Takeaway:** Independence means one event gives no information about the other, and mathematically it requires:\n",
        "\n",
        "$$\n",
        "P(E \\cap F) = P(E),P(F)\n",
        "$$\n",
        "\n",
        "Coin flips are a classic example of both independent and dependent event pairs depending on how the events are defined.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* Dependent events → information carries over\n",
        "* Independent events → no influence\n",
        "* Independence rule:\n",
        "\n",
        "$$\n",
        "P(E \\cap F) = P(E),P(F)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "dzj_NbP_zWoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4wgIdYzy3CA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Probability"
      ],
      "metadata": {
        "id": "f_z2xwOM0Y9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Conditional Probability?**\n",
        "\n",
        "Conditional probability lets us compute the chance of an event $E$ happening **given that** another event $F$ has happened. This is essential when events are not independent and information changes the probability.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Independence vs Conditional Probability**\n",
        "\n",
        "If events $E$ and $F$ are independent:\n",
        "\n",
        "$$P(E \\cap F) = P(E)\\,P(F)$$\n",
        "\n",
        "If they are not necessarily independent (and $P(F) \\neq 0$):\n",
        "\n",
        "$$P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}$$\n",
        "\n",
        "* Read $P(E \\mid F)$ as \"probability of $E$ given $F$\"\n",
        "\n",
        "**2. Equivalent Rearrangement (Useful Identity)**\n",
        "\n",
        "$$P(E \\cap F) = P(E \\mid F)\\,P(F)$$\n",
        "\n",
        "**3. Independence Implies Conditioning Doesn't Change Anything**\n",
        "\n",
        "If $E$ and $F$ are independent:\n",
        "\n",
        "$$P(E \\mid F) = P(E)$$\n",
        "\n",
        "Meaning: knowing $F$ happened gives no extra information about $E$\n",
        "\n",
        "---\n",
        "\n",
        "## Two-Child Example (Tricky Conditional Probability)\n",
        "\n",
        "**Why This Example?**\n",
        "\n",
        "It shows that conditional probability depends heavily on **what exactly you are told**, even if the situations seem similar.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Setup Assumptions**\n",
        "\n",
        "* Each child is equally likely to be a boy or girl\n",
        "* Children's genders are independent\n",
        "\n",
        "So probabilities are:\n",
        "\n",
        "* no girls: $1/4$\n",
        "* one girl + one boy: $1/2$\n",
        "* two girls: $1/4$\n",
        "\n",
        "**2. Conditional on \"Older Child is a Girl\"**\n",
        "\n",
        "Let:\n",
        "\n",
        "* $B$: both children are girls\n",
        "* $G$: older child is a girl\n",
        "\n",
        "Then:\n",
        "\n",
        "$$P(B \\mid G) = \\frac{P(B \\cap G)}{P(G)}$$\n",
        "\n",
        "Since $B \\cap G = B$, we get:\n",
        "\n",
        "$$P(B \\mid G) = \\frac{P(B)}{P(G)} = \\frac{1/4}{1/2} = \\frac{1}{2}$$\n",
        "\n",
        "**3. Conditional on \"At Least One Child is a Girl\"**\n",
        "\n",
        "Let:\n",
        "\n",
        "* $L$: at least one child is a girl\n",
        "\n",
        "Similarly $B \\cap L = B$, so:\n",
        "\n",
        "$$P(B \\mid L) = \\frac{P(B)}{P(L)}$$\n",
        "\n",
        "Now $P(L) = 3/4$, hence:\n",
        "\n",
        "$$P(B \\mid L) = \\frac{1/4}{3/4} = \\frac{1}{3}$$\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "* Given \"at least one girl,\" families with one girl + one boy are **twice as likely** as families with two girls.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Simulation to Verify the Result\n",
        "\n",
        "**Why Simulate?**\n",
        "\n",
        "Simulation confirms the theoretical probabilities by generating many random families and counting outcomes.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Generate Random Kids**\n",
        "```python\n",
        "import enum, random\n",
        "\n",
        "class Kid(enum.Enum):\n",
        "    BOY = 0\n",
        "    GIRL = 1\n",
        "\n",
        "def random_kid() -> Kid:\n",
        "    return random.choice([Kid.BOY, Kid.GIRL])\n",
        "```\n",
        "\n",
        "**2. Run Trials and Estimate Probabilities**\n",
        "```python\n",
        "both_girls = 0\n",
        "older_girl = 0\n",
        "either_girl = 0\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "for _ in range(10000):\n",
        "    younger = random_kid()\n",
        "    older = random_kid()\n",
        "\n",
        "    if older == Kid.GIRL:\n",
        "        older_girl += 1\n",
        "    if older == Kid.GIRL and younger == Kid.GIRL:\n",
        "        both_girls += 1\n",
        "    if older == Kid.GIRL or younger == Kid.GIRL:\n",
        "        either_girl += 1\n",
        "\n",
        "print(\"P(both | older):\", both_girls / older_girl)   # ~ 1/2\n",
        "print(\"P(both | either):\", both_girls / either_girl) # ~ 1/3\n",
        "```\n",
        "\n",
        "* Results match expectation:\n",
        "  * $P(B \\mid G) \\approx 1/2$\n",
        "  * $P(B \\mid L) \\approx 1/3$\n",
        "\n",
        "---\n",
        "\n",
        "**Takeaway:** Conditional probability depends on what information you condition on. Even similar-sounding facts (\"older child is a girl\" vs \"at least one is a girl\") can produce different results.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* Conditional probability: $P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}$\n",
        "\n",
        "* Product rule: $P(E \\cap F) = P(E \\mid F)\\,P(F)$\n",
        "\n",
        "* Independence implies: $P(E \\mid F) = P(E)$"
      ],
      "metadata": {
        "id": "Z9AIrffs0Zxf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--xsd4x20ZaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayes’s Theorem (Reversing Conditional Probabilities)"
      ],
      "metadata": {
        "id": "_FoJUy6l01OA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Why Bayes's Theorem?**\n",
        "\n",
        "Bayes's theorem helps you compute $P(E \\mid F)$ when you instead know $P(F \\mid E)$. It's a core tool for reasoning under uncertainty (especially when base rates matter).\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Start from Conditional Probability**\n",
        "\n",
        "$$P(E \\mid F) = \\frac{P(E \\cap F)}{P(F)}$$\n",
        "\n",
        "$$P(F \\mid E) = \\frac{P(E \\cap F)}{P(E)}$$\n",
        "\n",
        "Rearranging gives:\n",
        "\n",
        "$$P(E \\mid F) = \\frac{P(F \\mid E)\\,P(E)}{P(F)}$$\n",
        "\n",
        "**2. Expand $P(F)$ Using Two Mutually Exclusive Cases**\n",
        "\n",
        "Event $F$ can happen either with $E$ or with $\\neg E$:\n",
        "\n",
        "$$P(F) = P(F \\cap E) + P(F \\cap \\neg E)$$\n",
        "\n",
        "Substitute:\n",
        "\n",
        "$$P(E \\mid F) = \\frac{P(F \\mid E)\\,P(E)}{P(F \\mid E)\\,P(E) + P(F \\mid \\neg E)\\,P(\\neg E)}$$\n",
        "\n",
        "This is the common Bayes form.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Medical Test Example (Base Rate Fallacy)\n",
        "\n",
        "**Why This Example?**\n",
        "\n",
        "It shows that even a highly accurate test can produce many false positives when the disease is very rare.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Define Events**\n",
        "\n",
        "* $T$: test is positive\n",
        "* $D$: person has the disease\n",
        "\n",
        "We want: $P(D \\mid T)$\n",
        "\n",
        "**2. Plug Into Bayes's Theorem**\n",
        "\n",
        "$$P(D \\mid T) = \\frac{P(T \\mid D)\\,P(D)}{P(T \\mid D)\\,P(D) + P(T \\mid \\neg D)\\,P(\\neg D)}$$\n",
        "\n",
        "Given:\n",
        "* $P(D) = 1/10{,}000 = 0.0001$\n",
        "* $P(\\neg D) = 0.9999$\n",
        "* $P(T \\mid D) = 0.99$\n",
        "* $P(T \\mid \\neg D) = 0.01$\n",
        "\n",
        "Result:\n",
        "* $P(D \\mid T) \\approx 0.98\\%$ — So less than 1% of positive tests actually indicate disease.\n",
        "\n",
        "**3. Intuition with 1,000,000 People**\n",
        "\n",
        "* Expected diseased: $100$\n",
        "  * positives among them: $99$\n",
        "* Expected not diseased: $999{,}900$\n",
        "  * false positives among them: $9{,}999$\n",
        "\n",
        "So positive tests total ($99 + 9{,}999$), and only $99$ are real:\n",
        "\n",
        "$$\\frac{99}{99 + 9999} \\approx 0.98\\%$$\n",
        "\n",
        "**4. Important Assumption**\n",
        "\n",
        "This assumes people take the test randomly. If mostly symptomatic people test, the probability would be higher because you'd be conditioning on more information.\n",
        "\n",
        "---\n",
        "\n",
        "**Takeaway:** Bayes's theorem shows why base rates matter. Even accurate tests can have low \"true positive probability\" when the condition is rare.\n",
        "\n",
        "**Key Formulas:**\n",
        "\n",
        "$$P(E \\mid F) = \\frac{P(F \\mid E)\\,P(E)}{P(F)}$$\n",
        "\n",
        "$$P(E \\mid F) = \\frac{P(F \\mid E)\\,P(E)}{P(F \\mid E)\\,P(E) + P(F \\mid \\neg E)\\,P(\\neg E)}$$"
      ],
      "metadata": {
        "id": "wgsW4AOz1ATs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrGyRAIX0_zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Variable"
      ],
      "metadata": {
        "id": "MK7eGPhx1i6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Random Variables?**\n",
        "\n",
        "A random variable represents a quantity whose value is uncertain, and whose possible outcomes follow a probability distribution. Random variables are fundamental for modeling randomness in statistics and data science.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Definition of a Random Variable**\n",
        "\n",
        "A random variable is a variable whose possible values have an associated probability distribution.\n",
        "\n",
        "Examples:\n",
        "* Coin flip outcome: $1$ if heads, $0$ if tails\n",
        "* Number of heads in 10 flips\n",
        "* Uniform pick from `range(10)` (values 0–9 equally likely)\n",
        "\n",
        "**2. Probability Distribution Describes Outcome Likelihood**\n",
        "\n",
        "Examples:\n",
        "* Coin flip variable:\n",
        "  * $P(X=0)=0.5$\n",
        "  * $P(X=1)=0.5$\n",
        "* Uniform `range(10)` variable:\n",
        "  * $P(X=k)=0.1$ for each $k \\in \\{0,1,2,\\dots,9\\}$\n",
        "\n",
        "**3. Expected Value (Mean of a Random Variable)**\n",
        "\n",
        "Expected value is the probability-weighted average of outcomes.\n",
        "\n",
        "Coin flip example:\n",
        "\n",
        "$$\\mathbb{E}[X] = 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} = \\frac{1}{2}$$\n",
        "\n",
        "Uniform `range(10)` example:\n",
        "* Expected value is $4.5$\n",
        "\n",
        "**4. Random Variables Can Be Conditioned**\n",
        "\n",
        "Random variables can be defined given an event occurs, changing their distribution.\n",
        "\n",
        "Two-child example ($X$ = number of girls):\n",
        "* $P(X=0)=\\frac{1}{4}$\n",
        "* $P(X=1)=\\frac{1}{2}$\n",
        "* $P(X=2)=\\frac{1}{4}$\n",
        "\n",
        "Conditional variables:\n",
        "* $Y$ = number of girls given at least one child is a girl\n",
        "  * $P(Y=1)=\\frac{2}{3}$\n",
        "  * $P(Y=2)=\\frac{1}{3}$\n",
        "* $Z$ = number of girls given older child is a girl\n",
        "  * $P(Z=1)=\\frac{1}{2}$\n",
        "  * $P(Z=2)=\\frac{1}{2}$\n",
        "\n",
        "**5. Random Variables Often Appear Implicitly**\n",
        "\n",
        "* In practice, many data science methods use random variables even if not explicitly labeled as such\n",
        "* Looking deeper often reveals random-variable thinking behind the math\n",
        "\n",
        "---\n",
        "\n",
        "**Takeaway:** A random variable maps uncertain outcomes into numeric values with a probability distribution. You can compute expected values and update distributions using conditioning, which is key for probabilistic reasoning.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* Random variable → numeric outcomes + distribution\n",
        "* Distribution → probabilities for each value\n",
        "* Expected value: $$\\mathbb{E}[X] = \\sum_x x \\, P(X=x)$$\n",
        "* Conditioning changes the distribution of a random variable"
      ],
      "metadata": {
        "id": "5jW9QJwt1lSq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ITQVtPn21ktR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Distributions (PDF and CDF)"
      ],
      "metadata": {
        "id": "YURuYk4E8bi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Continuous Distributions?**\n",
        "\n",
        "Discrete distributions assign probability to distinct outcomes (like coin flips). But many real-world values vary continuously (like measurements), so we use **continuous distributions** over real numbers.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Discrete vs Continuous Distributions**\n",
        "\n",
        "* **Discrete distribution:** assigns positive probability to specific outcomes\n",
        "  Example: coin flip ($0$ or $1$)\n",
        "* **Continuous distribution:** outcomes lie on a continuum (infinitely many values)\n",
        "  Example: uniform distribution over $[0,1]$\n",
        "\n",
        "**2. Why Individual Points Have Probability 0**\n",
        "\n",
        "* Since there are infinitely many real numbers in an interval, a continuous distribution must assign:\n",
        "  * $P(X = x) = 0$ for any single point\n",
        "\n",
        "**3. Probability Density Function (PDF)**\n",
        "\n",
        "A continuous distribution is represented by a **PDF** $f(x)$, where probability comes from **area under the curve**.\n",
        "\n",
        "$$P(a \\le X \\le b) = \\int_a^b f(x)\\,dx$$\n",
        "\n",
        "Approximation intuition:\n",
        "\n",
        "$$P(x \\le X \\le x+h) \\approx h \\cdot f(x) \\quad \\text{(for small } h\\text{)}$$\n",
        "\n",
        "**4. Uniform Distribution PDF**\n",
        "```python\n",
        "def uniform_pdf(x: float) -> float:\n",
        "    return 1 if 0 <= x < 1 else 0\n",
        "```\n",
        "\n",
        "* Constant density of $1$ inside $[0,1)$, zero outside\n",
        "* Example: probability between $0.2$ and $0.3$ is $0.1$\n",
        "* `random.random()` behaves like a pseudo-random variable with this uniform density\n",
        "\n",
        "**5. Cumulative Distribution Function (CDF)**\n",
        "\n",
        "The **CDF** gives the probability that a random variable is less than or equal to $x$.\n",
        "\n",
        "$$F(x) = P(X \\le x)$$\n",
        "\n",
        "Uniform distribution CDF:\n",
        "```python\n",
        "def uniform_cdf(x: float) -> float:\n",
        "    \"\"\"Returns the probability that a uniform random variable is <= x\"\"\"\n",
        "    if x < 0:\n",
        "        return 0\n",
        "    elif x < 1:\n",
        "        return x\n",
        "    else:\n",
        "        return 1\n",
        "```\n",
        "\n",
        "* If $x < 0$, probability is $0$\n",
        "* If $0 \\le x < 1$, probability is $x$\n",
        "* If $x \\ge 1$, probability is $1$\n",
        "\n",
        "---\n",
        "\n",
        "**Takeaway:** Continuous distributions use a PDF to represent how probability is spread across intervals, and a CDF to represent accumulated probability up to a point.\n",
        "\n",
        "**Key Methods / Concepts:**\n",
        "\n",
        "* PDF: $$P(a \\le X \\le b) = \\int_a^b f(x)\\,dx$$\n",
        "\n",
        "* CDF: $$F(x) = P(X \\le x)$$\n",
        "\n",
        "* `uniform_pdf(x)` → uniform density on $[0,1)$\n",
        "* `uniform_cdf(x)` → probability up to $x$ for uniform distribution"
      ],
      "metadata": {
        "id": "kLBp4_8s8eK3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdWcwFV08cDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Normal Distribution (PDF, CDF, Standardization, Inverse CDF)"
      ],
      "metadata": {
        "id": "FDW42Pg-8_P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Normal Distribution?**\n",
        "\n",
        "The normal distribution is the classic **bell-shaped** curve used throughout statistics and ML. It is fully defined by two parameters:\n",
        "\n",
        "* mean $\\mu$ (center)\n",
        "* standard deviation $\\sigma$ (spread)\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Normal Distribution Parameters**\n",
        "\n",
        "* $\\mu$ controls where the curve is centered\n",
        "* $\\sigma$ controls how wide or narrow the curve is\n",
        "  * larger $\\sigma$ → wider and flatter\n",
        "  * smaller $\\sigma$ → narrower and taller\n",
        "\n",
        "**2. Probability Density Function (PDF)**\n",
        "\n",
        "Colab-friendly formula:\n",
        "\n",
        "$$f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "Python implementation:\n",
        "```python\n",
        "import math\n",
        "\n",
        "SQRT_TWO_PI = math.sqrt(2 * math.pi)\n",
        "\n",
        "def normal_pdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
        "    return (math.exp(-(x - mu) ** 2 / (2 * sigma ** 2)) / (SQRT_TWO_PI * sigma))\n",
        "```\n",
        "\n",
        "**3. Plotting Normal PDFs (Different Shapes)**\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xs = [x / 10.0 for x in range(-50, 50)]\n",
        "\n",
        "plt.plot(xs, [normal_pdf(x, sigma=1) for x in xs], '-',  label='mu=0,sigma=1')\n",
        "plt.plot(xs, [normal_pdf(x, sigma=2) for x in xs], '--', label='mu=0,sigma=2')\n",
        "plt.plot(xs, [normal_pdf(x, sigma=0.5) for x in xs], ':', label='mu=0,sigma=0.5')\n",
        "plt.plot(xs, [normal_pdf(x, mu=-1) for x in xs], '-.', label='mu=-1,sigma=1')\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Various Normal pdfs\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**4. Standard Normal + Standardization**\n",
        "\n",
        "* Standard normal: $\\mu = 0$, $\\sigma = 1$\n",
        "\n",
        "If $Z \\sim \\mathcal{N}(0,1)$, then:\n",
        "\n",
        "$$X = \\sigma Z + \\mu$$\n",
        "\n",
        "gives $X \\sim \\mathcal{N}(\\mu, \\sigma)$\n",
        "\n",
        "Conversely, if $X \\sim \\mathcal{N}(\\mu,\\sigma)$, then:\n",
        "\n",
        "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
        "\n",
        "turns it into a standard normal variable.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Cumulative Distribution Function (CDF)**\n",
        "\n",
        "The normal CDF has no simple closed-form, but can be computed using the error function `erf`.\n",
        "```python\n",
        "def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n",
        "    return (1 + math.erf((x - mu) / (math.sqrt(2) * sigma))) / 2\n",
        "```\n",
        "\n",
        "**6. Plotting Normal CDFs**\n",
        "```python\n",
        "xs = [x / 10.0 for x in range(-50, 50)]\n",
        "\n",
        "plt.plot(xs, [normal_cdf(x, sigma=1) for x in xs], '-',  label='mu=0,sigma=1')\n",
        "plt.plot(xs, [normal_cdf(x, sigma=2) for x in xs], '--', label='mu=0,sigma=2')\n",
        "plt.plot(xs, [normal_cdf(x, sigma=0.5) for x in xs], ':', label='mu=0,sigma=0.5')\n",
        "plt.plot(xs, [normal_cdf(x, mu=-1) for x in xs], '-.', label='mu=-1,sigma=1')\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.title(\"Various Normal cdfs\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**7. Inverse Normal CDF (Quantiles) via Binary Search**\n",
        "\n",
        "Sometimes we need the value $x$ such that:\n",
        "\n",
        "$$P(X \\le x) = p$$\n",
        "\n",
        "Since there's no simple inverse formula, we approximate it using binary search:\n",
        "```python\n",
        "def inverse_normal_cdf(p: float,\n",
        "                       mu: float = 0,\n",
        "                       sigma: float = 1,\n",
        "                       tolerance: float = 1e-5) -> float:\n",
        "    \"\"\"Find approximate inverse using binary search\"\"\"\n",
        "\n",
        "    # if not standard, compute standard and rescale\n",
        "    if mu != 0 or sigma != 1:\n",
        "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
        "\n",
        "    low_z = -10.0\n",
        "    hi_z = 10.0\n",
        "\n",
        "    while hi_z - low_z > tolerance:\n",
        "        mid_z = (low_z + hi_z) / 2\n",
        "        mid_p = normal_cdf(mid_z)\n",
        "\n",
        "        if mid_p < p:\n",
        "            low_z = mid_z\n",
        "        else:\n",
        "            hi_z = mid_z\n",
        "\n",
        "    return mid_z\n",
        "```\n",
        "\n",
        "* `normal_cdf` is strictly increasing, so binary search works reliably\n",
        "* It repeatedly narrows the interval until the probability matches closely\n",
        "\n",
        "---\n",
        "\n",
        "**Takeaway:** The normal distribution is controlled by $\\mu$ and $\\sigma$, can be standardized to/from the standard normal, and while its CDF has no elementary formula, it can be computed using `erf` and inverted numerically with binary search.\n",
        "\n",
        "**Key Methods / Concepts:**\n",
        "\n",
        "* Normal PDF: $$f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "* Standardization: $$Z = \\frac{X-\\mu}{\\sigma}$$\n",
        "\n",
        "* `normal_pdf(x, mu, sigma)`\n",
        "* `normal_cdf(x, mu, sigma)`\n",
        "* `inverse_normal_cdf(p, mu, sigma)`"
      ],
      "metadata": {
        "id": "DqJQ3-Pj9ADr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNSU_3jX8_w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Central Limit Theorem (CLT)"
      ],
      "metadata": {
        "id": "KekwgKLF9tMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why the CLT Matters?**\n",
        "\n",
        "The Central Limit Theorem explains why the **normal distribution** appears everywhere: averages (or sums) of many independent random variables tend to look normal, even if the original variables are not.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Core Idea of CLT**\n",
        "\n",
        "If $x_1, x_2, \\dots, x_n$ are **independent and identically distributed (i.i.d.)** with mean $\\mu$ and standard deviation $\\sigma$, then when $n$ is large:\n",
        "\n",
        "The average:\n",
        "\n",
        "$$\\frac{1}{n}\\left(x_1 + x_2 + \\cdots + x_n\\right)$$\n",
        "\n",
        "is approximately normal with:\n",
        "\n",
        "* mean $\\mu$\n",
        "* standard deviation $\\frac{\\sigma}{\\sqrt{n}}$\n",
        "\n",
        "So:\n",
        "\n",
        "$$\\frac{1}{n}\\sum_{i=1}^{n} x_i \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)$$\n",
        "\n",
        "**2. Standardized (Often More Useful) Form**\n",
        "\n",
        "Equivalently, the standardized sum:\n",
        "\n",
        "$$\\frac{(x_1 + \\cdots + x_n) - n\\mu}{\\sigma\\sqrt{n}}$$\n",
        "\n",
        "is approximately:\n",
        "\n",
        "$$\\mathcal{N}(0,1)$$\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Binomial as a CLT Example\n",
        "\n",
        "**Why Binomial?**\n",
        "\n",
        "A binomial random variable is literally the sum of many Bernoulli trials, making it a perfect CLT demonstration.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "**1. Bernoulli Trial**\n",
        "\n",
        "A Bernoulli($p$) random variable:\n",
        "\n",
        "* equals $1$ with probability $p$\n",
        "* equals $0$ with probability $1-p$\n",
        "```python\n",
        "def bernoulli_trial(p: float) -> int:\n",
        "    \"\"\"Returns 1 with probability p and 0 with probability 1-p\"\"\"\n",
        "    return 1 if random.random() < p else 0\n",
        "```\n",
        "\n",
        "**2. Binomial Random Variable**\n",
        "\n",
        "Binomial($n,p$) = sum of $n$ Bernoulli($p$) trials:\n",
        "```python\n",
        "def binomial(n: int, p: float) -> int:\n",
        "    \"\"\"Returns the sum of n bernoulli(p) trials\"\"\"\n",
        "    return sum(bernoulli_trial(p) for _ in range(n))\n",
        "```\n",
        "\n",
        "**3. Mean and Standard Deviation**\n",
        "\n",
        "For Bernoulli($p$):\n",
        "\n",
        "* mean $= p$\n",
        "* standard deviation: $$\\sqrt{p(1-p)}$$\n",
        "\n",
        "For Binomial($n,p$):\n",
        "\n",
        "* mean: $$\\mu = np$$\n",
        "\n",
        "* standard deviation: $$\\sigma = \\sqrt{np(1-p)}$$\n",
        "\n",
        "**4. Normal Approximation to Binomial**\n",
        "\n",
        "CLT implies that for large $n$:\n",
        "\n",
        "$$\\text{Binomial}(n,p) \\approx \\mathcal{N}\\left(np,\\sqrt{np(1-p)}\\right)$$\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Visualization (Binomial vs Normal Approximation)\n",
        "\n",
        "**Why Plot It?**\n",
        "\n",
        "Plotting shows how closely the binomial histogram matches the normal curve when $n$ is large.\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "def binomial_histogram(p: float, n: int, num_points: int) -> None:\n",
        "    \"\"\"Picks points from a Binomial(n, p) and plots their histogram\"\"\"\n",
        "    data = [binomial(n, p) for _ in range(num_points)]\n",
        "\n",
        "    histogram = Counter(data)\n",
        "    plt.bar([x - 0.4 for x in histogram.keys()],\n",
        "            [v / num_points for v in histogram.values()],\n",
        "            0.8,\n",
        "            color='0.75')\n",
        "\n",
        "    mu = p * n\n",
        "    sigma = math.sqrt(n * p * (1 - p))\n",
        "\n",
        "    xs = range(min(data), max(data) + 1)\n",
        "    ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma)\n",
        "          for i in xs]\n",
        "\n",
        "    plt.plot(xs, ys)\n",
        "    plt.title(\"Binomial Distribution vs. Normal Approximation\")\n",
        "    plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Practical Use (Approximating Probabilities)\n",
        "\n",
        "**Why This Is Useful?**\n",
        "\n",
        "Computing binomial probabilities directly can be hard, but normal probabilities are easier to work with.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Probability of getting more than 60 heads in 100 fair coin flips:\n",
        "\n",
        "Instead of:\n",
        "\n",
        "* Binomial($100, 0.5$)\n",
        "\n",
        "Approximate with:\n",
        "\n",
        "$$\\mathcal{N}(50, 5)$$\n",
        "\n",
        "since:\n",
        "\n",
        "* mean $= 100 \\cdot 0.5 = 50$\n",
        "* standard deviation: $$\\sqrt{100 \\cdot 0.5 \\cdot 0.5} = 5$$\n",
        "\n",
        "---\n",
        "\n",
        "**Takeaway:** The CLT explains why sums and averages often look normal. It lets you approximate complex distributions (like binomial) with the normal distribution, making probability calculations much easier.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* Average of i.i.d. variables becomes normal: $$\\frac{1}{n}\\sum_{i=1}^{n} x_i \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)$$\n",
        "\n",
        "* Binomial normal approximation: $$\\text{Binomial}(n,p) \\approx \\mathcal{N}\\left(np,\\sqrt{np(1-p)}\\right)$$"
      ],
      "metadata": {
        "id": "agzJMSOv9t-k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8XacKq39tra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}